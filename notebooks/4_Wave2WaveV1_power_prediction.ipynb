{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from deepseries.models import Wave2WaveV1\n",
    "from deepseries.dataset import Property, TimeSeries, Seq2SeqDataLoader\n",
    "from deepseries.nn.loss import MSELoss, RMSELoss\n",
    "from deepseries.train import Learner\n",
    "from deepseries.optim import ReduceCosineAnnealingLR\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import chinese_calendar as calendar\n",
    "import datetime as dt\n",
    "info = pd.read_excel(\"../data/info.xlsx\")\n",
    "recored = info.set_index(\"contributor_id\")['huangzf']\n",
    "info = pd.read_excel(\"../data/info.xlsx\").set_index(\"contributor_id\")[['pjt_name', 'pjt_type']]\n",
    "norm_score = pd.read_csv(r\"../data/20200315_20200415.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = pd.read_csv('../data/df.csv', parse_dates=['data_time'])[['data_time', 'cid', 'value']]\n",
    "power = power.set_index(\"data_time\").groupby(\"cid\").resample(\"1H\").sum().reset_index()\n",
    "power = power.pivot(index='cid', columns='data_time', values='value')\n",
    "power = power.apply(np.log1p).iloc[:, 10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_zero = power.values == 0\n",
    "is_nan = power.isnull().values\n",
    "is_valid = ~is_zero & ~is_nan\n",
    "\n",
    "xy = np.ma.masked_array(power.values, mask=~is_valid)\n",
    "\n",
    "series_mu = xy.mean(axis=1).data.reshape(-1, 1)\n",
    "series_std = xy.std(axis=1).data.reshape(-1, 1)\n",
    "xy = (xy - series_mu) / series_std\n",
    "xy = xy.filled(0.)\n",
    "\n",
    "xy = np.expand_dims(xy, 1).astype('float32')\n",
    "\n",
    "N_TEST = 24 * 30\n",
    "N_VALID = 24 * 2\n",
    "DROP_ZERO = True\n",
    "DEC_LEN = 24 * 2\n",
    "ENC_LEN = 24 * 7\n",
    "time_free_space = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_lag(series, n):\n",
    "    lag = np.zeros_like(series)\n",
    "    lag[:, :, n:] = series[:, :, :-n]\n",
    "    return lag\n",
    "\n",
    "x_lag7 = n_lag(xy, 7 * 24)\n",
    "x_lag14 = n_lag(xy, 14 * 24)\n",
    "\n",
    "x_is_valid = np.expand_dims(is_valid, 1)\n",
    "\n",
    "x_num_features = np.concatenate([x_lag7, x_lag14, x_is_valid], axis=1).astype(\"float32\")\n",
    "\n",
    "weights = x_is_valid.astype(\"float32\") + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_feature(x, T):\n",
    "    psin = np.sin(x * np.pi * 2 / T)\n",
    "    pcos = np.cos(x * np.pi * 2 / T)\n",
    "    return np.stack([psin, pcos], axis=0)\n",
    "\n",
    "\n",
    "xy_weekday = np.repeat(\n",
    "    np.expand_dims(\n",
    "        periodic_feature(power.columns.weekday.values, 7), axis=0), xy.shape[0], axis=0)\n",
    "\n",
    "xy_hour = np.repeat(\n",
    "    np.expand_dims(\n",
    "        periodic_feature(power.columns.hour.values, 24), axis=0), xy.shape[0], axis=0)\n",
    "\n",
    "xy_month = np.repeat(\n",
    "    np.expand_dims(\n",
    "        periodic_feature(power.columns.month.values, 12), axis=0), xy.shape[0], axis=0)\n",
    "\n",
    "def get_holiday_features(dts):\n",
    "    holidays = pd.get_dummies(pd.Series(dts).apply(lambda x: calendar.get_holiday_detail(x)[1]))\n",
    "    holidays['sick'] = np.where((power.columns >= \"2020-02-01\") & (power.columns < \"2020-03-01\"), 1, 0)\n",
    "    return holidays\n",
    "\n",
    "holidays = get_holiday_features(power.columns)\n",
    "holidays = np.expand_dims(holidays.values.transpose(1, 0), 0)\n",
    "holidays = np.repeat(holidays, xy.shape[0], axis=0)\n",
    "\n",
    "xy_num_features = np.concatenate([\n",
    "    xy_weekday,\n",
    "    xy_hour,\n",
    "    xy_month,\n",
    "    holidays\n",
    "], axis=1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_cat_features = np.expand_dims(np.arange(62), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardSpliter:\n",
    "    \n",
    "    def split(self, time_idx, enc_len, dec_len, valid_size):\n",
    "        if valid_size < 1:\n",
    "            valid_size = int(np.floor(len(time_idx) * valid_size))\n",
    "        valid_idx = time_idx[-(valid_size+enc_len):]\n",
    "        train_idx = time_idx[:-valid_size]\n",
    "        return train_idx, valid_idx\n",
    "    \n",
    "spliter = ForwardSpliter()\n",
    "train_idx, valid_idx = spliter.split(np.arange(xy.shape[2]), ENC_LEN, DEC_LEN, N_TEST+N_VALID)\n",
    "valid_idx, test_idx = spliter.split(valid_idx, ENC_LEN, DEC_LEN, N_TEST)\n",
    "\n",
    "train_xy = TimeSeries(xy[:, :, train_idx])\n",
    "valid_xy = TimeSeries(xy[:, :, valid_idx])\n",
    "\n",
    "train_xy_features = TimeSeries(xy_num_features[:, :, train_idx])\n",
    "valid_xy_features = TimeSeries(xy_num_features[:, :, valid_idx])\n",
    "train_xy_cat = Property(xy_cat_features)\n",
    "\n",
    "train_x_features = TimeSeries(x_num_features[:, :, train_idx])\n",
    "valid_x_features = TimeSeries(x_num_features[:, :, valid_idx])\n",
    "valid_xy_cat = Property(xy_cat_features)\n",
    "\n",
    "train_weight = TimeSeries(weights[:, :, train_idx])\n",
    "valid_weight = TimeSeries(weights[:, :, valid_idx])\n",
    "\n",
    "train_frame = Seq2SeqDataLoader(train_xy, batch_size=16, enc_lens=ENC_LEN, dec_lens=DEC_LEN, use_cuda=True, mode='train', time_free_space=24,\n",
    "                          enc_num_feats=[train_xy_features, train_x_features], dec_num_feats=[train_xy_features], weights=train_weight,\n",
    "                               enc_cat_feats=[train_xy_cat], dec_cat_feats=[train_xy_cat])\n",
    "valid_frame = Seq2SeqDataLoader(valid_xy, batch_size=64, enc_lens=ENC_LEN, dec_lens=DEC_LEN, use_cuda=True, mode='train', time_free_space=0,\n",
    "                         time_interval=48, enc_num_feats=[valid_xy_features, valid_x_features], dec_num_feats=[valid_xy_features],\n",
    "                               weights=valid_weight, dec_cat_feats=[valid_xy_cat], enc_cat_feats=[valid_xy_cat])\n",
    "\n",
    "test_xy = xy[:, :, test_idx]\n",
    "test_xf = np.concatenate([xy_num_features[:, :, test_idx], x_num_features[:, :, test_idx]], axis=1)\n",
    "test_yf = xy_num_features[:, :, test_idx]\n",
    "test_dec_cat = np.repeat(np.expand_dims(xy_cat_features, 2), DEC_LEN, axis=2)\n",
    "test_enc_cat = np.repeat(np.expand_dims(xy_cat_features, 2), ENC_LEN, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[04/23/2020 16:30:21]] start training >>>>>>>>>>>  see log: tensorboard --logdir ./power_env\\logs\n",
      "[[04/23/2020 16:30:37]] epoch 1 / 1500, batch 100%, train loss 0.6934, valid loss 0.8635, cost 0.3 min\n",
      "[[04/23/2020 16:30:52]] epoch 2 / 1500, batch 100%, train loss 0.2070, valid loss 0.8207, cost 0.2 min\n",
      "[[04/23/2020 16:31:10]] epoch 3 / 1500, batch 100%, train loss 1.3414, valid loss 0.8928, cost 0.3 min\n",
      "[[04/23/2020 16:31:23]] epoch 4 / 1500, batch 100%, train loss 0.5467, valid loss 0.9882, cost 0.2 min\n",
      "[[04/23/2020 16:31:41]] epoch 5 / 1500, batch 100%, train loss 0.4340, valid loss 1.1661, cost 0.3 min\n",
      "[[04/23/2020 16:32:01]] epoch 6 / 1500, batch 100%, train loss 0.5371, valid loss 1.1690, cost 0.3 min\n",
      "[[04/23/2020 16:32:17]] epoch 7 / 1500, batch 100%, train loss 0.4245, valid loss 0.9368, cost 0.3 min\n",
      "[[04/23/2020 16:32:36]] epoch 8 / 1500, batch 100%, train loss 0.9840, valid loss 0.8528, cost 0.3 min\n",
      "[[04/23/2020 16:32:52]] epoch 9 / 1500, batch 100%, train loss 0.2079, valid loss 0.8230, cost 0.3 min\n",
      "[[04/23/2020 16:33:13]] epoch 10 / 1500, batch 100%, train loss 0.5128, valid loss 0.8155, cost 0.3 min\n",
      "[[04/23/2020 16:33:28]] epoch 11 / 1500, batch 100%, train loss 0.4481, valid loss 0.8217, cost 0.2 min\n",
      "[[04/23/2020 16:33:44]] epoch 12 / 1500, batch 100%, train loss 0.3391, valid loss 0.8161, cost 0.3 min\n",
      "[[04/23/2020 16:34:02]] epoch 13 / 1500, batch 100%, train loss 0.4143, valid loss 0.8119, cost 0.3 min\n",
      "[[04/23/2020 16:34:19]] epoch 14 / 1500, batch 100%, train loss 0.4905, valid loss 0.8105, cost 0.3 min\n",
      "[[04/23/2020 16:34:38]] epoch 15 / 1500, batch 100%, train loss 0.3785, valid loss 0.8042, cost 0.3 min\n",
      "[[04/23/2020 16:34:58]] epoch 16 / 1500, batch 100%, train loss 0.3658, valid loss 0.7868, cost 0.3 min\n",
      "[[04/23/2020 16:35:14]] epoch 17 / 1500, batch 100%, train loss 0.3541, valid loss 0.7532, cost 0.3 min\n",
      "[[04/23/2020 16:35:32]] epoch 18 / 1500, batch 100%, train loss 0.9741, valid loss 0.7302, cost 0.3 min\n",
      "[[04/23/2020 16:35:50]] epoch 19 / 1500, batch 100%, train loss 0.5690, valid loss 0.6660, cost 0.3 min\n",
      "[[04/23/2020 16:36:08]] epoch 20 / 1500, batch 100%, train loss 0.6904, valid loss 0.5644, cost 0.3 min\n",
      "[[04/23/2020 16:36:29]] epoch 21 / 1500, batch 100%, train loss 0.2545, valid loss 0.5552, cost 0.3 min\n",
      "[[04/23/2020 16:36:48]] epoch 22 / 1500, batch 100%, train loss 0.1712, valid loss 0.5199, cost 0.3 min\n",
      "[[04/23/2020 16:37:05]] epoch 23 / 1500, batch 100%, train loss 0.7486, valid loss 0.4805, cost 0.3 min\n",
      "[[04/23/2020 16:37:22]] epoch 24 / 1500, batch 100%, train loss 0.1421, valid loss 0.4403, cost 0.3 min\n",
      "[[04/23/2020 16:37:37]] epoch 25 / 1500, batch 100%, train loss 0.1491, valid loss 0.4209, cost 0.3 min\n",
      "[[04/23/2020 16:37:57]] epoch 26 / 1500, batch 100%, train loss 0.4874, valid loss 0.4180, cost 0.3 min\n",
      "[[04/23/2020 16:38:12]] epoch 27 / 1500, batch 100%, train loss 0.2488, valid loss 0.3971, cost 0.3 min\n",
      "[[04/23/2020 16:38:29]] epoch 28 / 1500, batch 100%, train loss 0.5410, valid loss 0.4083, cost 0.3 min\n",
      "[[04/23/2020 16:38:43]] epoch 29 / 1500, batch 100%, train loss 0.1302, valid loss 0.3967, cost 0.2 min\n",
      "[[04/23/2020 16:38:56]] epoch 30 / 1500, batch 100%, train loss 0.4986, valid loss 0.4092, cost 0.2 min\n",
      "[[04/23/2020 16:39:16]] epoch 31 / 1500, batch 100%, train loss 0.2783, valid loss 0.3821, cost 0.3 min\n",
      "[[04/23/2020 16:39:38]] epoch 32 / 1500, batch 100%, train loss 0.0856, valid loss 0.3927, cost 0.4 min\n",
      "[[04/23/2020 16:39:55]] epoch 33 / 1500, batch 100%, train loss 0.3285, valid loss 0.3899, cost 0.3 min\n",
      "[[04/23/2020 16:40:15]] epoch 34 / 1500, batch 100%, train loss 0.0788, valid loss 0.3869, cost 0.3 min\n",
      "[[04/23/2020 16:40:31]] epoch 35 / 1500, batch 100%, train loss 0.1868, valid loss 0.3920, cost 0.3 min\n",
      "[[04/23/2020 16:40:49]] epoch 36 / 1500, batch 100%, train loss 0.1205, valid loss 0.3834, cost 0.3 min\n",
      "[[04/23/2020 16:41:10]] epoch 37 / 1500, batch 100%, train loss 0.2397, valid loss 0.3732, cost 0.3 min\n",
      "[[04/23/2020 16:41:28]] epoch 38 / 1500, batch 100%, train loss 0.5012, valid loss 0.3755, cost 0.3 min\n",
      "[[04/23/2020 16:41:46]] epoch 39 / 1500, batch 100%, train loss 0.5909, valid loss 0.3916, cost 0.3 min\n",
      "[[04/23/2020 16:42:03]] epoch 40 / 1500, batch 100%, train loss 0.1636, valid loss 0.3585, cost 0.3 min\n",
      "[[04/23/2020 16:42:21]] epoch 41 / 1500, batch 100%, train loss 0.2544, valid loss 0.3592, cost 0.3 min\n",
      "[[04/23/2020 16:42:39]] epoch 42 / 1500, batch 100%, train loss 0.4882, valid loss 0.3874, cost 0.3 min\n",
      "[[04/23/2020 16:43:01]] epoch 43 / 1500, batch 100%, train loss 0.4646, valid loss 0.3939, cost 0.4 min\n",
      "[[04/23/2020 16:43:18]] epoch 44 / 1500, batch 100%, train loss 0.1897, valid loss 0.3961, cost 0.3 min\n",
      "[[04/23/2020 16:43:34]] epoch 45 / 1500, batch 100%, train loss 0.1496, valid loss 0.3955, cost 0.3 min\n",
      "[[04/23/2020 16:43:51]] epoch 46 / 1500, batch 100%, train loss 0.2552, valid loss 0.3728, cost 0.3 min\n",
      "[[04/23/2020 16:44:10]] epoch 47 / 1500, batch 100%, train loss 0.0264, valid loss 0.3651, cost 0.3 min\n",
      "[[04/23/2020 16:44:26]] epoch 48 / 1500, batch 100%, train loss 0.6657, valid loss 0.3556, cost 0.3 min\n",
      "[[04/23/2020 16:44:43]] epoch 49 / 1500, batch 100%, train loss 0.3056, valid loss 0.3521, cost 0.3 min\n",
      "[[04/23/2020 16:45:02]] epoch 50 / 1500, batch 100%, train loss 0.3096, valid loss 0.3532, cost 0.3 min\n",
      "[[04/23/2020 16:45:23]] epoch 51 / 1500, batch 100%, train loss 0.0652, valid loss 0.3611, cost 0.3 min\n",
      "[[04/23/2020 16:45:39]] epoch 52 / 1500, batch 100%, train loss 0.1010, valid loss 0.3617, cost 0.3 min\n",
      "[[04/23/2020 16:45:52]] epoch 53 / 1500, batch 100%, train loss 0.0845, valid loss 0.3568, cost 0.2 min\n",
      "[[04/23/2020 16:46:14]] epoch 54 / 1500, batch 100%, train loss 0.1846, valid loss 0.3709, cost 0.4 min\n",
      "[[04/23/2020 16:46:38]] epoch 55 / 1500, batch 100%, train loss 0.0953, valid loss 0.3703, cost 0.4 min\n",
      "[[04/23/2020 16:47:03]] epoch 56 / 1500, batch 100%, train loss 0.1611, valid loss 0.3713, cost 0.4 min\n",
      "[[04/23/2020 16:47:26]] epoch 57 / 1500, batch 100%, train loss 0.1964, valid loss 0.3766, cost 0.4 min\n",
      "[[04/23/2020 16:47:50]] epoch 58 / 1500, batch 100%, train loss 0.0744, valid loss 0.3758, cost 0.4 min\n",
      "[[04/23/2020 16:48:13]] epoch 59 / 1500, batch 100%, train loss 0.3352, valid loss 0.3802, cost 0.4 min\n",
      "[[04/23/2020 16:48:34]] epoch 60 / 1500, batch 100%, train loss 0.1342, valid loss 0.3775, cost 0.4 min\n",
      "[[04/23/2020 16:48:57]] epoch 61 / 1500, batch 100%, train loss 0.3007, valid loss 0.3826, cost 0.4 min\n",
      "[[04/23/2020 16:49:18]] epoch 62 / 1500, batch 100%, train loss 0.7617, valid loss 0.3735, cost 0.3 min\n",
      "[[04/23/2020 16:49:45]] epoch 63 / 1500, batch 100%, train loss 0.1843, valid loss 0.3699, cost 0.5 min\n",
      "[[04/23/2020 16:50:05]] epoch 64 / 1500, batch 100%, train loss 0.2305, valid loss 0.3686, cost 0.3 min\n",
      "[[04/23/2020 16:50:31]] epoch 65 / 1500, batch 100%, train loss 0.3622, valid loss 0.3694, cost 0.4 min\n",
      "[[04/23/2020 16:50:55]] epoch 66 / 1500, batch 100%, train loss 0.3354, valid loss 0.3680, cost 0.4 min\n",
      "[[04/23/2020 16:51:20]] epoch 67 / 1500, batch 100%, train loss 0.3515, valid loss 0.3696, cost 0.4 min\n",
      "[[04/23/2020 16:51:38]] epoch 68 / 1500, batch 100%, train loss 0.3077, valid loss 0.3611, cost 0.3 min\n",
      "[[04/23/2020 16:52:05]] epoch 69 / 1500, batch 100%, train loss 0.4821, valid loss 0.3668, cost 0.4 min\n",
      "[[04/23/2020 16:52:30]] epoch 70 / 1500, batch 100%, train loss 0.0900, valid loss 0.3644, cost 0.4 min\n",
      "[[04/23/2020 16:52:51]] epoch 71 / 1500, batch 100%, train loss 0.1216, valid loss 0.3675, cost 0.4 min\n",
      "[[04/23/2020 16:53:12]] epoch 72 / 1500, batch 100%, train loss 0.2795, valid loss 0.3679, cost 0.3 min\n",
      "[[04/23/2020 16:53:36]] epoch 73 / 1500, batch 100%, train loss 0.2195, valid loss 0.3699, cost 0.4 min\n",
      "[[04/23/2020 16:54:00]] epoch 74 / 1500, batch 100%, train loss 0.6678, valid loss 0.3768, cost 0.4 min\n",
      "[[04/23/2020 16:54:22]] epoch 75 / 1500, batch 100%, train loss 0.1265, valid loss 0.3691, cost 0.4 min\n",
      "[[04/23/2020 16:54:43]] epoch 76 / 1500, batch 100%, train loss 0.9180, valid loss 0.3639, cost 0.3 min\n",
      "[[04/23/2020 16:55:03]] epoch 77 / 1500, batch 100%, train loss 0.1025, valid loss 0.3632, cost 0.3 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[04/23/2020 16:55:20]] epoch 78 / 1500, batch 100%, train loss 0.1569, valid loss 0.3653, cost 0.3 min\n",
      "[[04/23/2020 16:55:38]] epoch 79 / 1500, batch 100%, train loss 0.4178, valid loss 0.3714, cost 0.3 min\n",
      "[[04/23/2020 16:55:58]] epoch 80 / 1500, batch 100%, train loss 0.2783, valid loss 0.3677, cost 0.3 min\n",
      "[[04/23/2020 16:56:16]] epoch 81 / 1500, batch 100%, train loss 0.1286, valid loss 0.3659, cost 0.3 min\n",
      "[[04/23/2020 16:56:30]] epoch 82 / 1500, batch 100%, train loss 0.2962, valid loss 0.3639, cost 0.2 min\n",
      "[[04/23/2020 16:56:44]] epoch 83 / 1500, batch 100%, train loss 0.5680, valid loss 0.3554, cost 0.2 min\n",
      "[[04/23/2020 16:57:01]] epoch 84 / 1500, batch 100%, train loss 0.1947, valid loss 0.3652, cost 0.3 min\n",
      "[[04/23/2020 16:57:13]] epoch 85 / 1500, batch 100%, train loss 0.3223, valid loss 0.3632, cost 0.2 min\n",
      "[[04/23/2020 16:57:33]] epoch 86 / 1500, batch 100%, train loss 0.1556, valid loss 0.3615, cost 0.3 min\n",
      "[[04/23/2020 16:57:49]] epoch 87 / 1500, batch 100%, train loss 0.1140, valid loss 0.3662, cost 0.3 min\n",
      "[[04/23/2020 16:58:05]] epoch 88 / 1500, batch 100%, train loss 0.1900, valid loss 0.3850, cost 0.3 min\n",
      "[[04/23/2020 16:58:18]] epoch 89 / 1500, batch 100%, train loss 0.1118, valid loss 0.3971, cost 0.2 min\n",
      "[[04/23/2020 16:58:35]] epoch 90 / 1500, batch 100%, train loss 0.6954, valid loss 0.3782, cost 0.3 min\n",
      "[[04/23/2020 16:58:50]] epoch 91 / 1500, batch 100%, train loss 0.0700, valid loss 0.3710, cost 0.3 min\n",
      "[[04/23/2020 16:59:05]] epoch 92 / 1500, batch 100%, train loss 0.0864, valid loss 0.3638, cost 0.2 min\n",
      "[[04/23/2020 16:59:21]] epoch 93 / 1500, batch 100%, train loss 0.0958, valid loss 0.3664, cost 0.3 min\n",
      "[[04/23/2020 16:59:37]] epoch 94 / 1500, batch 100%, train loss 0.4680, valid loss 0.3591, cost 0.3 min\n",
      "[[04/23/2020 16:59:55]] epoch 95 / 1500, batch 100%, train loss 0.0437, valid loss 0.3541, cost 0.3 min\n",
      "[[04/23/2020 17:00:12]] epoch 96 / 1500, batch 100%, train loss 0.1438, valid loss 0.3541, cost 0.3 min\n",
      "[[04/23/2020 17:00:28]] epoch 97 / 1500, batch 100%, train loss 0.1403, valid loss 0.3516, cost 0.3 min\n",
      "[[04/23/2020 17:00:45]] epoch 98 / 1500, batch 100%, train loss 0.3501, valid loss 0.3391, cost 0.3 min\n",
      "[[04/23/2020 17:01:01]] epoch 99 / 1500, batch 100%, train loss 0.3120, valid loss 0.3347, cost 0.3 min\n",
      "[[04/23/2020 17:01:18]] epoch 100 / 1500, batch 100%, train loss 0.2005, valid loss 0.3738, cost 0.3 min\n",
      "[[04/23/2020 17:01:36]] epoch 101 / 1500, batch 100%, train loss 0.2408, valid loss 0.3731, cost 0.3 min\n",
      "[[04/23/2020 17:01:52]] epoch 102 / 1500, batch 100%, train loss 0.1112, valid loss 0.3505, cost 0.3 min\n",
      "[[04/23/2020 17:02:07]] epoch 103 / 1500, batch 100%, train loss 0.3456, valid loss 0.3209, cost 0.2 min\n",
      "[[04/23/2020 17:02:21]] epoch 104 / 1500, batch 100%, train loss 0.4591, valid loss 0.3160, cost 0.2 min\n",
      "[[04/23/2020 17:02:40]] epoch 105 / 1500, batch 100%, train loss 0.7804, valid loss 0.3134, cost 0.3 min\n",
      "[[04/23/2020 17:02:57]] epoch 106 / 1500, batch 100%, train loss 0.1337, valid loss 0.3337, cost 0.3 min\n",
      "[[04/23/2020 17:03:14]] epoch 107 / 1500, batch 100%, train loss 0.0851, valid loss 0.3548, cost 0.3 min\n",
      "[[04/23/2020 17:03:29]] epoch 108 / 1500, batch 100%, train loss 0.1171, valid loss 0.3816, cost 0.3 min\n",
      "[[04/23/2020 17:03:45]] epoch 109 / 1500, batch 100%, train loss 0.1028, valid loss 0.3863, cost 0.3 min\n",
      "[[04/23/2020 17:04:04]] epoch 110 / 1500, batch 100%, train loss 0.3103, valid loss 0.3412, cost 0.3 min\n",
      "[[04/23/2020 17:04:18]] epoch 111 / 1500, batch 100%, train loss 0.3876, valid loss 0.3726, cost 0.2 min\n",
      "[[04/23/2020 17:04:30]] epoch 112 / 1500, batch 100%, train loss 0.1410, valid loss 0.4054, cost 0.2 min\n",
      "[[04/23/2020 17:04:47]] epoch 113 / 1500, batch 100%, train loss 0.1332, valid loss 0.3998, cost 0.3 min\n",
      "[[04/23/2020 17:05:05]] epoch 114 / 1500, batch 100%, train loss 0.1377, valid loss 0.3957, cost 0.3 min\n",
      "[[04/23/2020 17:05:24]] epoch 115 / 1500, batch 100%, train loss 0.6838, valid loss 0.4008, cost 0.3 min\n",
      "[[04/23/2020 17:05:43]] epoch 116 / 1500, batch 100%, train loss 0.2995, valid loss 0.3756, cost 0.3 min\n",
      "[[04/23/2020 17:05:59]] epoch 117 / 1500, batch 100%, train loss 0.0694, valid loss 0.3833, cost 0.3 min\n",
      "[[04/23/2020 17:06:15]] epoch 118 / 1500, batch 100%, train loss 0.2407, valid loss 0.3707, cost 0.3 min\n",
      "[[04/23/2020 17:06:35]] epoch 119 / 1500, batch 100%, train loss 0.3679, valid loss 0.3407, cost 0.3 min\n",
      "[[04/23/2020 17:06:50]] epoch 120 / 1500, batch 100%, train loss 0.3008, valid loss 0.4015, cost 0.2 min\n",
      "[[04/23/2020 17:07:06]] epoch 121 / 1500, batch 100%, train loss 0.4165, valid loss 0.3449, cost 0.3 min\n",
      "[[04/23/2020 17:07:22]] epoch 122 / 1500, batch 100%, train loss 0.0657, valid loss 0.4264, cost 0.3 min\n",
      "[[04/23/2020 17:07:36]] epoch 123 / 1500, batch 100%, train loss 0.5057, valid loss 0.4308, cost 0.2 min\n",
      "[[04/23/2020 17:07:51]] epoch 124 / 1500, batch 100%, train loss 0.2597, valid loss 0.3685, cost 0.2 min\n",
      "[[04/23/2020 17:08:08]] epoch 125 / 1500, batch 100%, train loss 0.4034, valid loss 0.4167, cost 0.3 min\n",
      "[[04/23/2020 17:08:20]] epoch 126 / 1500, batch 100%, train loss 0.1511, valid loss 0.4531, cost 0.2 min\n",
      "[[04/23/2020 17:08:39]] epoch 127 / 1500, batch 100%, train loss 0.1539, valid loss 0.4665, cost 0.3 min\n",
      "[[04/23/2020 17:09:00]] epoch 128 / 1500, batch 100%, train loss 0.2602, valid loss 0.4256, cost 0.3 min\n",
      "[[04/23/2020 17:09:14]] epoch 129 / 1500, batch 100%, train loss 0.1876, valid loss 0.3542, cost 0.2 min\n",
      "[[04/23/2020 17:09:29]] epoch 130 / 1500, batch 100%, train loss 0.4376, valid loss 0.3248, cost 0.2 min\n",
      "[[04/23/2020 17:09:44]] epoch 131 / 1500, batch 100%, train loss 0.2164, valid loss 0.3587, cost 0.2 min\n",
      "[[04/23/2020 17:10:06]] epoch 132 / 1500, batch 100%, train loss 0.4839, valid loss 0.3939, cost 0.4 min\n",
      "[[04/23/2020 17:10:20]] epoch 133 / 1500, batch 100%, train loss 0.1479, valid loss 0.4113, cost 0.2 min\n",
      "[[04/23/2020 17:10:35]] epoch 134 / 1500, batch 100%, train loss 0.3060, valid loss 0.3436, cost 0.3 min\n",
      "[[04/23/2020 17:10:49]] epoch 135 / 1500, batch 100%, train loss 0.1152, valid loss 0.3125, cost 0.2 min\n",
      "[[04/23/2020 17:11:11]] epoch 136 / 1500, batch 100%, train loss 0.4060, valid loss 0.3515, cost 0.4 min\n",
      "[[04/23/2020 17:11:32]] epoch 137 / 1500, batch 100%, train loss 0.3902, valid loss 0.3300, cost 0.3 min\n"
     ]
    }
   ],
   "source": [
    "model = Wave2WaveV1(enc_num=9+8, dec_num=6+8, n_layers=8, n_blocks=3, \n",
    "                enc_cat=[(63, 4)], dec_cat=[(63, 4)], dropout=0.1, debug=False, hidden_size=512)\n",
    "opt = Adam(model.parameters(), 0.002)\n",
    "loss_fn = MSELoss()\n",
    "model.cuda()\n",
    "lr_scheduler = ReduceCosineAnnealingLR(opt, 64)\n",
    "learner = Learner(model, opt, loss_fn, './power_env', verbose=5000, lr_scheduler=lr_scheduler)\n",
    "learner.fit(1500, train_frame, valid_frame, patient=64, start_save=1, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xy = torch.as_tensor(xy[:, :, test_idx]).cuda()\n",
    "test_xf = torch.as_tensor(np.concatenate([xy_num_features[:, :, test_idx], x_num_features[:, :, test_idx]], axis=1)).cuda()\n",
    "test_yf = torch.as_tensor(xy_num_features[:, :, test_idx]).cuda()\n",
    "\n",
    "def plot(x_true, y_true, y_pred):\n",
    "    enc_ticks = np.arange(x_true.shape[1])\n",
    "    dec_ticks = np.arange(y_pred.shape[1]) + x_true.shape[1]\n",
    "    for idx, name in enumerate(power.index):\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(enc_ticks, x_true[idx])\n",
    "        plt.plot(dec_ticks, y_pred[idx], label='pred')\n",
    "        plt.plot(dec_ticks, y_true[idx], label='true')\n",
    "        plt.title(name)\n",
    "        plt.legend()\n",
    "\n",
    "def wmape(y_hat, y):\n",
    "    scores = []\n",
    "    for day in range(int(y.shape[0] / 24)):\n",
    "        scores.append(np.abs(y[day*24: (day+1)*24] - y_hat[day*24: (day+1)*24]).sum() / np.sum(y[day*24: (day+1)*24]))\n",
    "    return scores\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    scores = {}\n",
    "    for idx, name in enumerate(power.index):\n",
    "        scores[name] = wmape(y_pred[idx], y_true[idx])\n",
    "    return pd.DataFrame(scores)\n",
    "\n",
    "def wmape_dataframe(y_hat, y):\n",
    "    scores = []\n",
    "    for day in range(int(y.shape[0] / 96)):\n",
    "        scores.append(np.abs(y[day*96: (day+1)*96] - y_hat[day*96: (day+1)*96]).sum() / np.sum(y[day*96: (day+1)*96]))\n",
    "    return scores\n",
    "\n",
    "def metric_dataframe(y_true, y_pred):\n",
    "    scores = {}\n",
    "    for idx, name in enumerate(power.index):\n",
    "        scores[name] = wmape_dataframe(y_pred.iloc[idx], y_true.iloc[idx])\n",
    "    return pd.DataFrame(scores)\n",
    "\n",
    "def predict(learner, xy, x_feats, y_feats, epoch):\n",
    "    learner.load(epoch)\n",
    "    learner.model.eval()\n",
    "    learner.model.cuda()\n",
    "    preds = []\n",
    "    days = int(xy.shape[2] / 24 - ENC_LEN / 24 - DEC_LEN/24 + 1)\n",
    "    for day in range(days):\n",
    "        step = day * 24\n",
    "#         enc_start = day\n",
    "#         enc_end = (step+ENC_LEN) / 24\n",
    "#         dec_start = enc_end\n",
    "#         dec_end = (step+ENC_LEN+DEC_LEN) / 24\n",
    "#         print(f\"start {enc_start}, end {int(dec_end)}\" )\n",
    "        step_pred = model(\n",
    "            xy[:, :, step: step+ENC_LEN], \n",
    "            enc_num=x_feats[:, :, step: step+ENC_LEN],\n",
    "            dec_num=y_feats[:, :, step+ENC_LEN: step+ENC_LEN+DEC_LEN], dec_len=DEC_LEN).cpu().detach().numpy()\n",
    "        if step == 0:\n",
    "            preds.append(step_pred)\n",
    "        else:\n",
    "            preds.append(step_pred[:, :, -24:])\n",
    "    preds = np.concatenate(preds, axis=2)\n",
    "    preds = np.expm1(preds.squeeze() * series_std + series_mu)\n",
    "    \n",
    "    x_true = np.expm1(xy[:, :, :ENC_LEN].cpu().numpy().squeeze() * series_std + series_mu)\n",
    "    y_true = np.expm1(xy[:, :, ENC_LEN:].cpu().numpy().squeeze() * series_std + series_mu)\n",
    "    \n",
    "    return x_true, y_true, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = pd.read_csv(\"./data/20200315_20200415.csv\").drop(['Unnamed: 0', 'model_name'], axis=1)\n",
    "norm_data = norm_data[norm_data.contributor_id.isin(power.index)].reset_index(drop=True)\n",
    "norm_data = norm_data.set_index(\"contributor_id\").loc[power.index].reset_index()\n",
    "norm_data['data_time'] = pd.to_datetime(norm_data.data_time)\n",
    "norm_data = norm_data.set_index(\"data_time\").groupby(\"contributor_id\").resample('1H')[['forecast_pwr', 'value']].sum().reset_index()\n",
    "norm_true = norm_data.pivot(index='contributor_id', columns='data_time', values='value').iloc[:, 48:]\n",
    "norm_pred = norm_data.pivot(index='contributor_id', columns='data_time', values='forecast_pwr').iloc[:, 48:]\n",
    "\n",
    "\n",
    "x_true, y_true, y_pred  = predict(learner, test_xy, test_xf, test_yf, 1014)\n",
    "scores = pd.DataFrame([metric(y_true, y_pred).mean().rename(\"wave\"), \n",
    "                       metric(norm_true.values, norm_pred.values).mean().rename(\"v1\")]).T.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_true, y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
